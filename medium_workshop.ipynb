{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_Iff6KLKc2b"
      },
      "source": [
        "## Building and improving a Multi-layer percepton : Feed-forward neural network using real-word data\n",
        "## Overview\n",
        "\n",
        "The dataset is Electronic Health Record Predicting collected from a private Hospital in Indonesia. It contains the patients laboratory test results used to determine next patient treatment whether in care or out care patient.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFqO6GR4LyOK"
      },
      "source": [
        "This workshop , is gonna be divided into 5 steps :\n",
        "\n",
        "\n",
        "\n",
        "1.   Exploratory Data Analysis (EDA)\n",
        "2.   Feature Engineering\n",
        "3.   Building our MLP Model\n",
        "3.   Attemping to increase accuracy and model performace with Feature Selection\n",
        "4.   Attemping to increase accuracy and model performance with Feature Scaling\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-8aaMY96XEi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score ,classification_report\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwWZuT6XjNhu"
      },
      "source": [
        "# Data Preprocessing\n",
        "## Exploratory Data Analysis\n",
        "\n",
        "In the next following steps , we are gonna apply simples concepts of data analysis.\n",
        "In this part we are gonna use libraries such as pandas , matplotlib , numpy.\n",
        "The main goal we to summarize their main importance\n",
        "It is essential before every decision-making to have a look at your dataset and understanding each attributes , this includes :\n",
        "\n",
        "*   Attribute Name and Type\n",
        "*   Measurement unit\n",
        "\n",
        "**Important note :**\n",
        "\n",
        "A feature is an individual measurable property within a recorded dataset. In machine learning and statistics, features are often called “variables” or “attributes.” Relevant features have a correlation or bearing (called feature importance) on a model's use case.\n",
        "\n",
        "### Reading data\n",
        "\n",
        "To be able to read data from our dataset file , we import a library called pandas.\n",
        "This library allow us to not only read the file but to display the information that comes with it.\n",
        "\n",
        "By using **.head()** , it returs by default the first 5 rows of our dataset.\n",
        "\n",
        "When using **.head(n)** , it returns the first n rows of our dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrWwyBoi6d07"
      },
      "outputs": [],
      "source": [
        "patient_data = pd.read_csv('data-ori.csv')\n",
        "\n",
        "patient_data.head() # return the first 5 rows by default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZ_d8nmHkrFw"
      },
      "outputs": [],
      "source": [
        "patient_data.head(3) # return the first n rows by default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4hjPAHhk3D5"
      },
      "source": [
        "The next step would be displayig , the info of our dataset attributes to have a better perpestive of how the data is organized.\n",
        "\n",
        "When using **.info()** , it returns a consice summary of our dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f34_ylJ7lcQZ"
      },
      "outputs": [],
      "source": [
        "patient_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "452xh88uld5V"
      },
      "source": [
        "After looking at our dataset summary , we can observe the following attributes :\n",
        "\n",
        "* **HAEMOTOCRIT** - Patient laboratory test result of haematocrit;\n",
        "* **HAEMOGLOBINS** - Patient laboratory test result of haemoglobins;\n",
        "* **ERYTHROCYTE** - Patient laboraty test result of erythrocyte;\n",
        "* **LEUCOCYTE** - Patient laboratory test result of leucocyte;\n",
        "* **THROMOBOCYTE** - Patient laboratory test result of thrombocyte;\n",
        "* **MCH** - Patient laboratory test result of MCH , it shows the average amount of hemoglobin in a cell;\n",
        "* **MCHC** - Patient laboratory test result of MCHC , it shows the measurement of the amount of hemoglobin a red blood cell has relative to the size of the cell;\n",
        "* **MCV** - Patient laboratory test result of MCV , it shows measures the average size of your red blood cells;\n",
        "* **AGE** - Patient Age;\n",
        "* **SEX** - Patient Gender;\n",
        "* **SOURCE** - Patient In Care or Out Care."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e96JGbD1_Zfx"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "\n",
        "\n",
        "There are a few essential generalise key steps we need to follow before looking at our neural network, applying feature engineering alows to extract features from raw data with some domain knowldge\n",
        "In this specefic workshop we are gonna go through the following steps :\n",
        "\n",
        "\n",
        "\n",
        "1.   Identify if there are NaN Values\n",
        "2.   Identify the number of distinc elements\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq4iadrEAM4m"
      },
      "source": [
        "When using **isna()** and **.sum()** , we are checking if in our dataset there are any NaN values.\n",
        "NaN values stand for Not a Number , and its one of the major problems of data analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CDkwd1lPOZJ"
      },
      "outputs": [],
      "source": [
        "patient_data.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6OgxrI7n03W"
      },
      "source": [
        "When using **.nunique()** , we are checking the number of distinct elements in a specific axes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXgdd_tfQO2-"
      },
      "outputs": [],
      "source": [
        "patient_data.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UX4nDUdoF1P"
      },
      "source": [
        "### Visualing statistical data of our dataset\n",
        "\n",
        "When using **.describe()** , it returns a statistical summary of our dataset.\n",
        "\n",
        "\n",
        "\n",
        "*   **count** -> corresponds to the number of non-NA/null observations\n",
        "*   **mean** -> corresponds to the mean of the observations (the sum of all values divided by the total number of values)\n",
        "*   **std** -> corresponds to the standard deviation of the observations (measure of the amount of variation of a random variable expected about its mean)\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "573dtmpXQU6L"
      },
      "outputs": [],
      "source": [
        "patient_data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byxa91o7oZ_0"
      },
      "source": [
        "## Categorical Data Encoding\n",
        "\n",
        "When looking at our dataset , attributes type , we can see that both 'SOURCE' and 'SEX' , have **object** as a Type.\n",
        "When dealing with\n",
        "Deep learning neural networks require that input and output variables are numbers meaning that categorical data must be encoded to numbers before we can use it to fit and evaluate a model.\n",
        "In this case we are gonna use a binary enconding startegy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMdawaiQo0A6"
      },
      "source": [
        "In this case , we are gonna attribute 'In' to the numeric value of 1 (True) and 'Out' to the numeric value of 0 (False)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQKtSUFBQYBa"
      },
      "outputs": [],
      "source": [
        "patient_data['SOURCE'] = patient_data.SOURCE.replace({\"in\":1, 'out':0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeTtpP17eLkO"
      },
      "outputs": [],
      "source": [
        "patient_data['SEX'] = patient_data['SEX'].replace({'F': 0, 'M': 1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC-jyqfmpFbm"
      },
      "source": [
        "After applying our data transformation , we can analyse the outcome of it.\n",
        "When using **.value_counts()** , it returns the different type of possible values present in that column and the ammount of rows it is present"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJlaVe-WQwiZ"
      },
      "outputs": [],
      "source": [
        "patient_data.SOURCE.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5F-O5p4pgel"
      },
      "source": [
        "## Plotting our data\n",
        "\n",
        "After dealing with data management , we can plot our data to have a better inpretation of how it behaves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYnqNY7rREGK"
      },
      "outputs": [],
      "source": [
        "\n",
        "slices = [patient_data.shape[0] - patient_data['SOURCE'].sum() , patient_data['SOURCE'].sum()]\n",
        "labels = ['In-care', 'Out-care']\n",
        "explode = [0, 0.1]\n",
        "\n",
        "plt.pie(slices, labels=labels, explode=explode, shadow=True,\n",
        "        startangle=90, autopct='%1.1f%%',\n",
        "        wedgeprops={'edgecolor': 'black'})\n",
        "\n",
        "plt.title(\"Patient Classification\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5m6x7E1pfhr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEn_BIGJT6vC"
      },
      "outputs": [],
      "source": [
        "patient_data.SEX.value_counts() ## Returns the number of enteties , on the Gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSsAa-bfT-RG"
      },
      "outputs": [],
      "source": [
        "patient_data.SEX.value_counts().plot(kind='bar')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYMZxBM0qNzn"
      },
      "source": [
        "## Patient Classfication\n",
        "\n",
        "To be able to predict a patient classification , we need to define that attribute as our target.\n",
        "The same attribute will be separated from the rest of the others.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHHiXA5-XmTy"
      },
      "outputs": [],
      "source": [
        "feat = [f for f in patient_data.columns if f !='SOURCE']\n",
        "\n",
        "y = patient_data['SOURCE']\n",
        "x = patient_data.drop('SOURCE', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-61EksPyX0u3"
      },
      "outputs": [],
      "source": [
        "print(f\"The dataset contains {patient_data.shape[0]} rows and {patient_data.shape[1]} columns\")\n",
        "\n",
        "num_feat = [f for f in feat if patient_data[f].dtype != object]\n",
        "cat_feat = [f for f in feat if patient_data[f].dtype == object]\n",
        "\n",
        "print(f\"Total number of features : {len(feat)}\")\n",
        "print(f\"Number of numerical features : {len(num_feat)}\")\n",
        "print(f\"Number of categorical features : {len(cat_feat)}\\n\")\n",
        "\n",
        "patient_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l22AYQSqknt"
      },
      "source": [
        "## Attribute Behaviour\n",
        "\n",
        "Plotting each attribute behaviour according to our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fE_btAdYIPC"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(9, 1, figsize=(8, 25))\n",
        "for i, c in enumerate(num_feat):\n",
        "    f = patient_data[[c]].plot(kind='kde',ax=axes[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLju69w5rtBL"
      },
      "source": [
        "## Data Training/Testing (70%/30%)\n",
        "\n",
        "\n",
        "In Deep Learning, training and testing data are essential components used to develop and evaluate predictive models. Here's a brief explanation of each:\n",
        "\n",
        "Training Data:\n",
        "\n",
        "Training data is used to train a machine learning model. During training, the model learns patterns, relationships, and features from the input data to make predictions or classifications.\n",
        "It consists of a labeled dataset, where both the input features and the corresponding correct output (target) are known. The model adjusts its parameters based on this labeled data to minimize the difference between its predictions and the actual outcomes.\n",
        "The model iteratively updates its internal parameters through optimization algorithms, adjusting its behavior to capture the underlying patterns in the training data.\n",
        "\n",
        "\n",
        "Testing Data:\n",
        "\n",
        "Testing data is used to assess the model's performance and generalization ability. Once the model is trained, it is evaluated on this separate dataset to measure how well it can make predictions on new, unseen data.\n",
        "Similar to training data, testing data includes input features, but the corresponding target values are kept hidden during the evaluation process. This allows the model to be tested on its ability to generalize and make accurate predictions for new, previously unseen examples.\n",
        "The model's predictions on the testing data are compared against the actual outcomes to calculate performance metrics such as accuracy, precision, recall, and F1 score. These metrics help assess how well the model is expected to perform on new, real-world data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x,y,train_size = 0.7,\n",
        "                                                     shuffle = True,random_state = 1)"
      ],
      "metadata": {
        "id": "mpPKBCGgdnlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX4_-toVAQGO"
      },
      "source": [
        "# Multi-layer preception neural network\n",
        "\n",
        "A Multi-Layer Perceptron (MLP) is a type of artificial neural network with multiple layers of nodes. It's a feedforward network, processing information from input to output without loops. MLPs are used for tasks like classification and regression. Key components include:\n",
        "\n",
        "Input Layer: Represents input data features, with each node corresponding to a feature.\n",
        "\n",
        "Hidden Layers: Layers between input and output, applying weights, biases, and activation functions. Number and size of hidden layers are adjustable parameters.\n",
        "\n",
        "Weights and Bias: Connections between nodes have weights, learned during training. Nodes have bias terms for capturing non-origin patterns.\n",
        "\n",
        "Activation Function: Applies non-linearity to node outputs, like sigmoid, tanh, or ReLU, enabling the network to learn complex relationships.\n",
        "\n",
        "Feedforward Process: Input data passes through layers, with nodes processing input, applying weights, adding bias, and passing through activation functions to produce output.\n",
        "\n",
        "Output Layer: Produces final results based on task; e.g., one node with sigmoid for binary classification or multiple nodes with softmax for multiclass.\n",
        "\n",
        "Training: Adjusts weights and biases using backpropagation, minimizing the difference between predicted and actual values.\n",
        "\n",
        "Loss Function: Measures the difference between predicted and actual values; goal during training is to minimize this difference.\n",
        "\n",
        "MLPs are versatile neural networks suitable for diverse machine learning tasks, capable of learning intricate data relationships through non-linear processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp=MLPClassifier(hidden_layer_sizes=(150,100,50), max_iter=300,activation = 'relu',solver='adam',random_state=1)\n",
        "mlp.fit(x_train, y_train)\n",
        "\n",
        "y_pred = mlp.predict(x_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "K7TXQBEsT2Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoWuDOMZ8p90"
      },
      "source": [
        "# Accuracy Score:\n",
        "\n",
        "The accuracy curve , is also knwon as the training accuracy curve , show us how good the model is at making correct predictions on the training data as it goes through the trainign process.\n",
        "\n",
        "It is measure in percentages and gives us the proportion of instances the model correctly classified out of the total number of instances.\n",
        "\n",
        "The accuracy curve gives us a sense of how well the model fits the training data and improves its ability to make acccurate predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "6lZxQY3ZUAxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0-BSQSE9h-t"
      },
      "source": [
        "# F1 Score :\n",
        "\n",
        "As showing above there are a number of metrics useful for measuring performance of classification models.\n",
        "Accuracy is realiable as long as your dataset has an equal number of samples for each class.\n",
        "\n",
        "When having for example 98% of instances from Class A ( yes ) and 2% of samples from Class B ( no). Regardless of the instance of this type of dataset , the accuracy will still be 98%.\n",
        "\n",
        "When talking about imbalanced data , F1 score comes to rescue . It takes into account the type of errors - false positive and false negative - and not just the number of predictions that were incorrect.\n",
        "\n",
        "F1 scores can range from 0 to 1 , with 1 representing a model that perfectly classifies each observation into the correct class and 0 the opposite.\n",
        "\n",
        "F1 score is usefull to usi in a classifciation problem as it balances precison and recal.\n",
        "\n",
        "In Healthcare , high F1 scores indicates that the model is good at identifiying both positive and negative cases , minimazing misdiagnosis and ensure patients receive proper treatment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULSNIKBuwP1u"
      },
      "outputs": [],
      "source": [
        "\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", class_report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJzZDT3cBuf1"
      },
      "outputs": [],
      "source": [
        "f1 = f1_score(y_test,y_pred)\n",
        "print(\" F1 score : {:.5f}\".format(f1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attemping to improve our model accuracy\n",
        "In deep learning, feature selection is used to make the process more accurate. It also increases the prediction power of the algorithms by selecting the most critical variables and eliminating the redundant and irrelevant ones. This is why feature selection is important.\n",
        "\n",
        "Three key benefits of feature selection are:\n",
        "  \n",
        "\n",
        "*   Decreases over-fitting  \n",
        "*   Fewer redundant data means fewer chances of making decisions based on noise\n",
        "* Improves Accuracy  \n",
        "* Reduces Training Time  \n",
        "\n"
      ],
      "metadata": {
        "id": "j3I7vR0dUcoL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO-rVUg_qs2z"
      },
      "source": [
        "## Plotting the relationship between target (Y) and mean of each numerical features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPcinVNyYbRY"
      },
      "outputs": [],
      "source": [
        "  # Relationship between target and mean of each numerical features\n",
        "\n",
        "fig, axes = plt.subplots(5,2, figsize=(14,24))\n",
        "axes = [x for axes_row in axes for x in axes_row]\n",
        "for i,c in enumerate(patient_data[num_feat]):\n",
        "    df = patient_data.groupby(\"SOURCE\")[c].mean()\n",
        "    plot = df.plot(kind='bar', title=c, ax=axes[i], ylabel=f'Mean {c}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQkNnfOHq2GM"
      },
      "source": [
        "## Pearson Correlation\n",
        "\n",
        "The Pearson correlation coefficient (r) is the most common way of measuring a linear correlation. It is a number between –1 and 1 that measures the strength and direction of the relationship between two variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YU0wbJ0Zx6u"
      },
      "outputs": [],
      "source": [
        "corr_matt = patient_data.corr()[['SOURCE']].sort_values(by='SOURCE',ascending=False)\n",
        "plt.figure(figsize=(3,5))\n",
        "corr = sns.heatmap(corr_matt, annot=True, cmap='BrBG', cbar=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting the features you want to remove"
      ],
      "metadata": {
        "id": "_C2gJ0mkVjya"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afqtNkJLe0MD"
      },
      "outputs": [],
      "source": [
        "# Remove least correlated features\n",
        "\n",
        "features_to_remove = ['MCHC','MCH','MCV']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.drop(features_to_remove, axis=1, inplace=True)\n",
        "x_test.drop(features_to_remove, axis=1, inplace=True)\n",
        "\n",
        "for features in features_to_remove :\n",
        "  num_feat.remove(features)\n",
        "# final train set\n",
        "x_train.head()"
      ],
      "metadata": {
        "id": "yK_43QKsVYYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Proving if our model accuracy increase with feature selection"
      ],
      "metadata": {
        "id": "ZaudpY1sVqGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp=MLPClassifier(hidden_layer_sizes=(50,50), activation='relu', solver='adam', alpha=0.0001,\n",
        "                    learning_rate='adaptive', learning_rate_init=0.001, power_t=0.5, max_iter=400,\n",
        "                    random_state=0, tol=0.0001, verbose=10, warm_start=True,\n",
        "                    momentum=0.9, nesterovs_momentum=True, validation_fraction=0.2,\n",
        "                    beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=100)\n",
        "mlp.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = mlp.predict(x_test)\n"
      ],
      "metadata": {
        "id": "ghEZP_6iVvju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "ACAjnqLsVzss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", class_report)"
      ],
      "metadata": {
        "id": "ISWwikjtV5aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1 = f1_score(y_test,y_pred)\n",
        "print(\" F1 score : {:.5f}\".format(f1))"
      ],
      "metadata": {
        "id": "p1Kk94KsV59J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improving model performance (f1 score) by applying feature scaling"
      ],
      "metadata": {
        "id": "EtejhpGjYsjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Feature scaling in deep learning refers to the process of standardizing or normalizing the input features of a neural network. The goal is to ensure that all features contribute equally to the model's learning process and prevent certain features from dominating others due to differences in their scales. Common techniques for feature scaling include:"
      ],
      "metadata": {
        "id": "v2K3Cp57m8vO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we create the StandarScaler object, and then we perform .fit_transform(). It will calculate the mean(μ)and standard deviation(σ) of the feature F at a time it will transform the data points of the feature F."
      ],
      "metadata": {
        "id": "52vUS15Ia-zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(x_train)\n",
        "x_train[num_feat] = scaler.fit_transform(x_train[num_feat]) #fit and transform the train set\n",
        "x_test[num_feat] = scaler.transform(x_test[num_feat]) #transform the test test"
      ],
      "metadata": {
        "id": "4KWeNcubY0NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.head()"
      ],
      "metadata": {
        "id": "nd_v9XJviBCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building our final model\n",
        "\n",
        "To finish our workshop , we are gonna check if we were abble to improve our accuracy and model performance.\n",
        "Let´s start building our final MLP model.\n",
        "In this workshop you were able to apply the different techniques :\n",
        "\n",
        "*   Feature engineering\n",
        "*   Feature Selection\n",
        "*   Feature Scaling\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5JDpE0r9ZaRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp=MLPClassifier(hidden_layer_sizes=(50,50), activation='relu', solver='adam', alpha=0.0001, batch_size=1000,\n",
        "                    learning_rate='adaptive', learning_rate_init=0.001, power_t=0.5, max_iter=200,\n",
        "                    shuffle=True, random_state=0, tol=0.0001, verbose=10, warm_start=True,\n",
        "                    momentum=0.9, nesterovs_momentum=True, validation_fraction=0.2,\n",
        "                    beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=100)\n",
        "mlp.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = mlp.predict(x_test)"
      ],
      "metadata": {
        "id": "Zfkvk0hsZTkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "IW9rTXZ7ZWAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", class_report)"
      ],
      "metadata": {
        "id": "Sm68zZbmZXRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1 = f1_score(y_test,y_pred)\n",
        "print(\" F1 score : {:.5f}\".format(f1))"
      ],
      "metadata": {
        "id": "ZPftQ1AwZYcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eiQiCAnPNsb"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In our first model we achieved :\n",
        "\n",
        "\n",
        "*   Accuracy Score : **70% or 0.70**\n",
        "*   Model Performance (f1 score): **0.47632**\n",
        "\n",
        "After applying feature selection and feature scaling :\n",
        "\n",
        "\n",
        "*   Accuracy Score : **73% or 0.73**\n",
        "*   Model Performance (f1 score) : **0.65354**\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}